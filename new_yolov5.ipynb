{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading Roboflow workspace...\n",
      "loading Roboflow project...\n",
      "Downloading Dataset Version Zip in BATCAM-MX-Data-Labeling-1 to yolov5pytorch: 100% [17718066 / 17718066] bytes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Dataset Version Zip to BATCAM-MX-Data-Labeling-1 in yolov5pytorch:: 100%|██████████| 644/644 [00:00<00:00, 6633.28it/s]\n"
     ]
    }
   ],
   "source": [
    "from roboflow import Roboflow\n",
    "rf = Roboflow(api_key=\"j7Euw60KZdSImJYk7Kpy\")\n",
    "project = rf.workspace(\"batcam-mx\").project(\"batcam-mx-data-labeling\")\n",
    "dataset = project.version(1).download(\"yolov5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'yolov5'...\n"
     ]
    }
   ],
   "source": [
    "# YOLOv5 레퍼지토리 다운로드, Yolo 깃허브에서 checkpoint .pt 다운로드 (수동)\n",
    "! git clone https://github.com/ultralytics/yolov5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fusing layers... \n",
      "Model summary: 157 layers, 7026307 parameters, 0 gradients, 15.8 GFLOPs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to grab frame.\n",
      "Failed to grab frame.\n",
      "Failed to grab frame.\n",
      "Failed to grab frame.\n",
      "Failed to grab frame.\n",
      "Failed to grab frame.\n",
      "Failed to grab frame.\n",
      "Failed to grab frame.\n",
      "Failed to grab frame.\n",
      "Failed to grab frame.\n",
      "Failed to grab frame.\n",
      "Failed to grab frame.\n",
      "Failed to grab frame.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_19472\\2196293505.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[1;31m# Capture the webcam frame\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[1;31m# ret, webcam_frame = webcam_cap.read(0)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m     \u001b[0mwebcam_frame\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'image.png'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[1;31m# Check if frame read is valid\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Dahyun\\anaconda3\\envs\\23-2\\lib\\site-packages\\ultralytics\\utils\\patches.py\u001b[0m in \u001b[0;36mimread\u001b[1;34m(filename, flags)\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mThe\u001b[0m \u001b[0mread\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m     \"\"\"\n\u001b[1;32m---> 26\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfromfile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import yaml\n",
    "import sys\n",
    "\n",
    "# Modify sys.path\n",
    "# ** 패스 변경 필요\n",
    "sys.path.insert(0, \"./yolov5/\") # yolov5 패스지정\n",
    "\n",
    "# Now import attempt_load\n",
    "from yolov5.models.experimental import attempt_load\n",
    "\n",
    "# Load the \"custom\" YOLOv5 model\n",
    "# ** 0813 모델: 0812 데이터로 학습, 크기: 640x640\n",
    "model = attempt_load('./best_231016.pt')\n",
    "\n",
    "# Initialize the webcam capture\n",
    "# webcam_cap = cv2.VideoCapture(cv2.CAP_ANY)\n",
    "\n",
    "# Load class names from data.yaml\n",
    "with open('./yolov5/BATCAM-MX-Data-Labeling-1/data.yaml', 'r') as yaml_file:\n",
    "    data = yaml.safe_load(yaml_file)\n",
    "    class_names = data['names']\n",
    "\n",
    "while True:\n",
    "    # Capture the webcam frame\n",
    "    # ret, webcam_frame = webcam_cap.read(0)\n",
    "    webcam_frame = cv2.imread('image.png')\n",
    "    cv2.imshow('Webcam Capture', webcam_frame)\n",
    "    \n",
    "    # Check if frame read is valid\n",
    "    if not ret:\n",
    "        print(\"Failed to grab frame.\")\n",
    "        continue\n",
    "    \n",
    "    # Convert the webcam frame from BGR to RGB and reshape for model input\n",
    "    img = cv2.cvtColor(webcam_frame, cv2.COLOR_BGR2RGB)\n",
    "    img_tensor = torch.from_numpy(img).float().permute(2, 0, 1).unsqueeze(0) / 255.0\n",
    "    \n",
    "    # Pass the frame through the YOLOv5 model\n",
    "    results = model(img_tensor)\n",
    "\n",
    "    # Extract tensor from results tuple\n",
    "    detections = results[0]\n",
    "\n",
    "    # Assuming there's a confidence threshold you want to apply\n",
    "    conf_thresh = 0.6\n",
    "\n",
    "    # Use the confidence score to filter out weak detections\n",
    "    mask = detections[0, :, 4] > conf_thresh\n",
    "\n",
    "    # Extract the boxes, scores, and classes from the detections\n",
    "    boxes = detections[0, mask, :4].cpu().numpy()\n",
    "    scores = detections[0, mask, 4].cpu().numpy()\n",
    "    classes = detections[0, mask, 5].cpu().numpy().astype(np.int32)\n",
    "\n",
    "    # Draw the bounding boxes and labels on the frame\n",
    "    for box, score, class_idx in zip(boxes, scores, classes):\n",
    "        x1, y1, x2, y2 = map(int, box)\n",
    "        class_name = class_names[class_idx]\n",
    "        cv2.rectangle(webcam_frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "        cv2.putText(webcam_frame, f\"{class_name}: {score:.2f}\", (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "        \n",
    "        # Print the coordinates of the detected object\n",
    "        print(f\"{class_name} coordinates: ({x1}, {y1}), ({x2}, {y2})\")\n",
    "\n",
    "    # Display the frame on the screen\n",
    "    cv2.imshow('Webcam Capture', webcam_frame)\n",
    "\n",
    "    # Exit the program if the user presses the 'q' key\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the capture and close the window\n",
    "webcam_cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "23-2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
